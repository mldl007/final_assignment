{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb33998-e7fa-40ea-ae76-e6be81554dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd7b6c98-8956-43f5-9e44-7d42ba7c12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import fitz\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import yake\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef6793e-9048-439e-802a-ed164c11fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO CONVERT TEXT IN EACH PAGE TO TEXT AND CONCATENATING TEXT FROM EACH PAGE\n",
    "def pdf_to_text(path: str):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c752146-b1b6-41e0-9fd8-02be45b44ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSOR THAT REMOVES PUNCTUATIONS, DIGITS, STOP WORDS, SHORT WORDS AND LEMMATIZES THE TEXT\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, lemmatize: bool = True, remove_punct: bool = True, remove_digits: bool = True,\n",
    "                 remove_stop_words: bool = True,\n",
    "                 remove_short_words: bool = True, minlen: int = 1, maxlen: int = 1, top_p: float = None,\n",
    "                 bottom_p: float = None):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_digits = remove_digits\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_short_words = remove_short_words\n",
    "        self.minlen = minlen\n",
    "        self.maxlen = maxlen\n",
    "        self.top_p = top_p\n",
    "        self.bottom_p = bottom_p\n",
    "        self.words_to_remove = []\n",
    "        self.stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
    "                           \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                           'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "                           'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                           'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "                           'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "                           'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                           'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "                           'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
    "                           'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "                           'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                           'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\",\n",
    "                           'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                           'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                           \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n",
    "                           \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                           'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def __remove_double_whitespaces(string: str):\n",
    "        return \" \".join(string.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def __lemmatize(string_series: pd.Series):\n",
    "        nlp = spacy.load(os.path.join(\"..\", \"en_core_web_sm-3.4.1\"))\n",
    "\n",
    "        def str_lemmatize(string: str):\n",
    "            doc = nlp(string)\n",
    "            return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "        return string_series.map(str_lemmatize)\n",
    "\n",
    "    def __remove_punct(self, string_series: pd.Series):\n",
    "        clean_string_series = string_series.str.replace(pat=f'[{string.punctuation}]', repl=\" \", regex=True).copy()\n",
    "        return clean_string_series.map(self.__remove_double_whitespaces)\n",
    "\n",
    "    def __remove_digits(self, string_series: pd.Series):\n",
    "        clean_string_series = string_series.str.replace(pat=r'\\d', repl=\" \", regex=True).copy()\n",
    "        return clean_string_series.map(self.__remove_double_whitespaces)\n",
    "\n",
    "    @staticmethod\n",
    "    def __remove_short_words(string_series: pd.Series, minlen: int = 1, maxlen: int = 1):\n",
    "        clean_string_series = string_series.map(lambda string: \" \".join([word for word in string.split() if\n",
    "                                                                         (len(word) > maxlen) or (len(word) < minlen)]))\n",
    "        return clean_string_series\n",
    "\n",
    "    def __remove_stop_words(self, string_series: pd.Series):\n",
    "        def str_remove_stop_words(string: str):\n",
    "            stops = self.stop_words\n",
    "            return \" \".join([token for token in string.split() if token not in stops])\n",
    "\n",
    "        return string_series.map(str_remove_stop_words)\n",
    "\n",
    "    def __remove_top_bottom_words(self, string_series: pd.Series, top_p: int = None,\n",
    "                                  bottom_p: int = None, dataset: str = 'train'):\n",
    "        if dataset == 'train':\n",
    "            if top_p is None:\n",
    "                top_p = 0\n",
    "            if bottom_p is None:\n",
    "                bottom_p = 0\n",
    "\n",
    "            if top_p > 0 or bottom_p > 0:\n",
    "                word_freq = pd.Series(\" \".join(string_series).split()).value_counts()\n",
    "                n_words = len(word_freq)\n",
    "\n",
    "            if top_p > 0:\n",
    "                self.words_to_remove.extend([*word_freq.index[: int(np.ceil(top_p * n_words))]])\n",
    "\n",
    "            if bottom_p > 0:\n",
    "                self.words_to_remove.extend([*word_freq.index[-int(np.ceil(bottom_p * n_words)):]])\n",
    "\n",
    "        if len(self.words_to_remove) == 0:\n",
    "            return string_series\n",
    "        else:\n",
    "            clean_string_series = string_series.map(lambda string: \" \".join([word for word in string.split()\n",
    "                                                                             if word not in self.words_to_remove]))\n",
    "            return clean_string_series\n",
    "\n",
    "    def preprocess(self, string_series: pd.Series, dataset: str = \"train\"):\n",
    "        string_series = string_series.str.lower().copy()\n",
    "        if self.lemmatize:\n",
    "            string_series = self.__lemmatize(string_series=string_series)\n",
    "        if self.remove_punct:\n",
    "            string_series = self.__remove_punct(string_series=string_series)\n",
    "        if self.remove_digits:\n",
    "            string_series = self.__remove_digits(string_series=string_series)\n",
    "        if self.remove_stop_words:\n",
    "            string_series = self.__remove_stop_words(string_series=string_series)\n",
    "        if self.remove_short_words:\n",
    "            string_series = self.__remove_short_words(string_series=string_series,\n",
    "                                                      minlen=self.minlen,\n",
    "                                                      maxlen=self.maxlen)\n",
    "        string_series = self.__remove_top_bottom_words(string_series=string_series,\n",
    "                                                       top_p=self.top_p,\n",
    "                                                       bottom_p=self.bottom_p, dataset=dataset)\n",
    "\n",
    "        string_series = string_series.str.strip().copy()\n",
    "        string_series.replace(to_replace=\"\", value=\"this is an empty message\", inplace=True)\n",
    "\n",
    "        return string_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d8d968-41f2-46dc-961a-6679d96a54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT PDF TO TEXT AND RETURN PREPROCESSED DATA\n",
    "def text_clean(path: str):\n",
    "    text = pdf_to_text(path)\n",
    "    text_preprocessor = TextPreprocessor()\n",
    "    preprocessed_text = text_preprocessor.preprocess(pd.Series(text))[0]\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c12b14-ec49-45b5-8664-79e63e90ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = text_clean('daskcheatsheet.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32587c19-f21f-4ae5-a969-5c2eb6cbe129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dask parallel compute cheat sheet see full dask documentation http dask pydata org instruction use conda environment manager get http bit ly getconda dask quick install install dask conda install dask pip conda install dask pip install dask complete continue back dask collection easy use big datum collection dask dataframe parallel panda dataframe large datum import read csv datum read parquet data filter manipulate datum panda syntax standard groupby aggregation join etc compute result panda dataframe store csv parquet format example import dask dataframe dd df dd read csv data csv df dd read parquet data parquet df df df result df groupby df mean result compute result parquet output parquet df dd read csv filenames csv df groupby df timestamp day value mean compute import dask array da import py dataset py file data hdf group dataset da array dataset chunks da random uniform shape chunks dot mean axis result compute create dataset store py file data hdf da array path chunks mean axis create dataset store import dask bag db db sequence seq npartition db read text data json import json record map json loads filter lambda name alice records pluck key name mean compute records textfiles output json db read text bucket data json map json load filter lambda name alice textfiles bucket output json dask array parallel numpy array large datum import create array like object include hfd netcdf disk format alternatively generate array random distribution perform operation numpy syntax compute result numpy array store hdf netcdf disk format example dask bag parellel list unstructured datum import create dask bag sequence read text format map filter result compute aggregation like mean count sum store result back text format example dask collection continue custom computation custom code complex algorithms advance dask delay lazy parallelism custom code concurrent future asynchronous real time parallelism df dd read parquet bucket myfile parquet db read text hdfs path data json df df persist dask compute min max import dask dask delaye def load filename dask delaye def process data load dask delayed load process dask delayed process datum load fn fn filename result process datum dask compute result dask distribute import client client client future client submit func args kwargs result future result future completed future client submit read fn fn filename client submit process future future future client submit sum result future result read distribute file system cloud storage prepend prefix like hdfs gcs path persist lazy computation memory compute multiple output import wrap custom function dask delaye annotation delay function operate lazily produce task graph rather execute immediately pass delay result delay function create dependency task call function normal code compute result execute parallel import start local dask client submit individual task asynchronously block gather individual result process result arrive example user documentation technical documentation distribute scheduler report bug dask pydata org distributed readthedocs org github com dask dask issue resource set cluster launch cluster anaconda com info anaconda com manually single machine cloud deployment dask scheduler scheduler start scheduler address host dask worker scheduler address host dask worker scheduler address dask distribute import client client client scheduler address client client pip install dask kubernete pip install dask ec start scheduler one machine start worker machine provide address run scheduler start client python process call client argument easy setup single host see dask kubernete project google cloud see dask ec project amazon ec'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef7169f0-a74a-48b8-82d5-27a1eaacf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_word(text: str):\n",
    "    text = text.split()\n",
    "    return pd.Series(text).value_counts().index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900be94-b7c2-474e-8bd8-cdce0312b538",
   "metadata": {},
   "source": [
    "<h1>FREQUENT WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c017a6-ca02-41d7-89d3-cc8feeefb833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent word: dask\n"
     ]
    }
   ],
   "source": [
    "print(f'Frequent word: {find_frequent_word(clean_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cc337-bbce-4bd9-bdc3-781b8c93e68f",
   "metadata": {},
   "source": [
    "<H1>KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d2eee42-e3a2-4181-9bf3-611053455b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_extract = yake.KeywordExtractor(top=10, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99fd2c4c-1c9b-4cc0-8f7f-a92db8cf5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: dask, client, result, read, import, compute, future, scheduler, data, json\n"
     ]
    }
   ],
   "source": [
    "keywords = [key for key, _ in keyword_extract.extract_keywords(clean_text)]\n",
    "print(f'KEYWORDS: {\", \".join(keywords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0d842-d864-410f-9e6c-d7478c3169a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
